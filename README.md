# HackRx LLM Query Engine ğŸ‰ **100% FREE - No Payment Required!**

An intelligent document question-answering API that processes PDFs and answers questions using **free AI models**. Built for HackRx 6.0 with zero operational costs.

## ğŸš€ Quick Start

**Live API Endpoint:** `https://your-app.onrender.com/hackrx/run` *(Deploy instructions below)*

```bash
# Test the API
curl -X POST https://your-app.onrender.com/hackrx/run \
  -H "Authorization: Bearer hackrx-token" \
  -H "Content-Type: application/json" \
  -d '{
    "documents": "https://example.com/sample.pdf",
    "questions": ["What is this document about?", "What are the key points?"]
  }'

# Expected Response
{
  "answers": [
    "This document is about...",
    "The key points include..."
  ]
}
```

## âœ¨ Features

- ğŸ”— **Working API Endpoint**: `/hackrx/run` POST endpoint with Bearer auth
- ğŸ†“ **Zero Cost**: Uses free local AI models - no API fees!
- ï¿½ **PDF Processing**: Downloads and extracts text from any PDF URL
- ğŸ” **Semantic Search**: FAISS vector search for relevant content
- ğŸ¤– **AI-Powered**: Free sentence transformers + Hugging Face models
- âš¡ **Fast Response**: Optimized for <30s response time
- ğŸ“œ **Proper JSON**: Returns `{"answers": [...]}` format

## ğŸ›  Tech Stack

- **FastAPI**: Modern Python web framework
- **Sentence Transformers**: Free local embeddings (all-MiniLM-L6-v2)
- **Hugging Face**: Free text generation API
- **FAISS**: Efficient vector similarity search
- **PyMuPDF**: PDF text extraction
- **No paid APIs required!** ğŸ‰

## ğŸ Local Setup

### 1. Clone Repository
```bash
git clone https://github.com/SruthiJayaram/llm-query-engine-hackrx.git
cd llm-query-engine-hackrx
```

### 2. Install Dependencies
```bash
cd hackrx_app
pip install -r requirements.txt
```

### 3. Run Server
```bash
uvicorn main:app --host 0.0.0.0 --port 8000
```
**No API keys needed!** ğŸ‰ The app uses free local models.

### 4. Test Locally
```bash
# Health check
curl http://localhost:8000/health

# Full API test
curl -X POST http://localhost:8000/hackrx/run \
  -H "Authorization: Bearer test-token" \
  -H "Content-Type: application/json" \
  -d '{
    "documents": "https://www.w3.org/WAI/ER/tests/xhtml/testfiles/resources/pdf/dummy.pdf",
    "questions": ["What is this document about?"]
  }'
```

### 6. Test the API
```bash
# Health check
curl http://localhost:8000/health

# Test main endpoint
curl -X POST http://localhost:8000/hackrx/run \
  -H "Authorization: Bearer test-token-123" \
  -H "Content-Type: application/json" \
  -d '{"documents":"https://example.com/sample.pdf","questions":["What is this document about?"]}'
```

## ğŸš€ Deployment on Render.com

### 1. Push to GitHub
```bash
git add .
git commit -m "Ready for deployment"
git push origin main
```

### 2. Deploy on Render
1. Go to [render.com](https://render.com) and sign up/login
2. Click **"New +"** â†’ **"Web Service"**
3. Connect your GitHub repo: `SruthiJayaram/llm-query-engine-hackrx`
4. Configure settings:
   - **Name**: `hackrx-llm-engine` (or your choice)
   - **Build Command**: `cd hackrx_app && pip install -r requirements.txt`
   - **Start Command**: `cd hackrx_app && uvicorn main:app --host 0.0.0.0 --port $PORT`
   - **Environment Variables**: None needed! ğŸ‰
5. Click **"Deploy Web Service"**

### 3. Your Live URL
After deployment (~5-10 minutes), you'll get:
```
https://hackrx-llm-engine.onrender.com
```

Your HackRx endpoint will be:
```
https://hackrx-llm-engine.onrender.com/hackrx/run
```

## ğŸ“¡ API Usage

### Request Format
```bash
curl -X POST https://your-app.onrender.com/hackrx/run \
  -H "Authorization: Bearer <your-token>" \
  -H "Content-Type: application/json" \
  -d '{
    "documents": "https://example.com/document.pdf",
    "questions": [
      "What is the main topic?",
      "What are the key findings?",
      "What is the conclusion?"
    ]
  }'
```

### Response Format
```json
{
  "answers": [
    "The main topic is...",
    "The key findings include...",
    "The conclusion states that..."
  ]
}
```

### Validation
âœ… **Endpoint**: POST `/hackrx/run`  
âœ… **Authentication**: Bearer token required  
âœ… **Response Time**: <30 seconds  
âœ… **JSON Format**: `{"answers": [...]}`  
âœ… **PDF Processing**: Downloads & extracts text  
âœ… **AI Powered**: Free semantic search + text generation  

## ğŸ† HackRx 6.0 Submission

### Live Demo
**ğŸŒ Deployed API**: `https://your-app.onrender.com/hackrx/run`  
*(Replace with your actual Render URL after deployment)*

### Test Command
```bash
# Test the live API
curl -X POST https://your-app.onrender.com/hackrx/run \
  -H "Authorization: Bearer hackrx-demo-token" \
  -H "Content-Type: application/json" \
  -d '{
    "documents": "https://www.w3.org/WAI/ER/tests/xhtml/testfiles/resources/pdf/dummy.pdf",
    "questions": ["What is this document about?", "What type of content is it?"]
  }'
```

### Submission Checklist
- âœ… **Endpoint Path**: `/hackrx/run` (POST)
- âœ… **Authentication**: Bearer token validation
- âœ… **Response Format**: `{"answers": ["...", "..."]}`
- âœ… **PDF Processing**: Downloads and extracts text
- âœ… **AI-Powered**: Semantic search + text generation
- âœ… **Free Solution**: No paid APIs required
- âœ… **Fast Response**: Optimized for <30s
- âœ… **Production Ready**: Deployed on Render.com

### Architecture
1. **PDF Download**: Fetch document from URL
2. **Text Extraction**: PyMuPDF processes PDF content  
3. **Text Chunking**: Split into 500-word segments
4. **Embeddings**: sentence-transformers (local, free)
5. **Semantic Search**: FAISS finds relevant chunks
6. **Answer Generation**: Hugging Face API (free) + fallback
7. **JSON Response**: Structured answer array

## ğŸ“Š Performance

- **Response Time**: 5-25 seconds (includes model loading)
- **Accuracy**: Semantic search ensures relevant context
- **Cost**: $0 - completely free solution!
- **Reliability**: Local models + API fallbacks
- **Scalability**: Ready for production deployment

## Project Structure
```
hackrx_app/
â”œâ”€â”€ main.py          # FastAPI app and endpoints
â”œâ”€â”€ utils.py         # Core processing functions
â”œâ”€â”€ requirements.txt # Python dependencies
â”œâ”€â”€ .env            # Environment variables
â””â”€â”€ server.log      # Server logs (when running)
```

## How It Works

1. **Document Processing**: Downloads PDF from URL and extracts text
2. **Text Chunking**: Splits document into manageable chunks (500 words each)
3. **Embedding Creation**: Uses OpenAI's text-embedding-3-small to create vector embeddings
4. **Semantic Search**: Uses FAISS to find most relevant chunks for each question
5. **LLM Processing**: Sends relevant context + question to GPT-3.5-turbo for answer generation
6. **Response**: Returns structured JSON with all answers

## Testing
Use the included `test_api.sh` script:
```bash
./test_api.sh
```

## Error Handling
- âœ… Invalid/missing authorization tokens
- âœ… PDF download failures
- âœ… Text extraction errors
- âœ… OpenAI API errors
- âœ… Timeout handling (30s limit)

## Next Steps for Production
- [ ] Add caching for frequently accessed documents
- [ ] Implement async processing for multiple documents
- [ ] Add support for more document formats (DOCX, TXT)
- [ ] Upgrade to GPT-4 for better accuracy
- [ ] Add rate limiting and request validation
- [ ] Implement proper logging and monitoring
